{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2xq5vYzXPvhf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "import pickle\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading (수술 시 사망 데이터)\n",
        "data=pd.read_csv(\"https://raw.githubusercontent.com/GonieAhn/Data-Science-online-course-from-gonie/main/Data%20Store/example_data.csv\")"
      ],
      "metadata": {
        "id": "hHsV1LkaPzmM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "gw8wUlVSP1Ro",
        "outputId": "63134c8e-1783-4182-c2de-db2351cb36d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           censor        event         age        wtkg        hemo  \\\n",
              "count  532.000000   532.000000  532.000000  532.000000  532.000000   \n",
              "mean     0.340226   801.236842   35.225564   76.061855    0.078947   \n",
              "std      0.474231   326.887929    8.852094   13.224698    0.269910   \n",
              "min      0.000000    33.000000   13.000000   47.401000    0.000000   \n",
              "25%      0.000000   535.750000   29.000000   67.500000    0.000000   \n",
              "50%      0.000000   933.500000   34.000000   74.600000    0.000000   \n",
              "75%      1.000000  1081.000000   40.000000   83.502000    0.000000   \n",
              "max      1.000000  1231.000000   70.000000  149.000000    1.000000   \n",
              "\n",
              "             homo       drugs      karnof      oprior         z30  ...  \\\n",
              "count  532.000000  532.000000  532.000000  532.000000  532.000000  ...   \n",
              "mean     0.640977    0.118421   95.432331    0.030075    0.546992  ...   \n",
              "std      0.480165    0.323410    5.981856    0.170955    0.498255  ...   \n",
              "min      0.000000    0.000000   70.000000    0.000000    0.000000  ...   \n",
              "25%      0.000000    0.000000   90.000000    0.000000    0.000000  ...   \n",
              "50%      1.000000    0.000000  100.000000    0.000000    1.000000  ...   \n",
              "75%      1.000000    0.000000  100.000000    0.000000    1.000000  ...   \n",
              "max      1.000000    1.000000  100.000000    1.000000    1.000000  ...   \n",
              "\n",
              "           gender        str2       strat     symptom        cd40       cd420  \\\n",
              "count  532.000000  532.000000  532.000000  532.000000  532.000000  532.000000   \n",
              "mean     0.812030    0.580827    1.981203    0.167293  353.204887  336.139098   \n",
              "std      0.391056    0.493888    0.905946    0.373589  114.105253  130.961573   \n",
              "min      0.000000    0.000000    1.000000    0.000000  103.000000   49.000000   \n",
              "25%      1.000000    0.000000    1.000000    0.000000  271.000000  243.750000   \n",
              "50%      1.000000    1.000000    2.000000    0.000000  346.000000  330.500000   \n",
              "75%      1.000000    1.000000    3.000000    0.000000  422.000000  418.000000   \n",
              "max      1.000000    1.000000    3.000000    1.000000  771.000000  909.000000   \n",
              "\n",
              "            cd496           r         cd80        cd820  \n",
              "count  532.000000  532.000000   532.000000   532.000000  \n",
              "mean   173.146617    0.603383   987.250000   928.214286  \n",
              "std    191.455406    0.489656   475.223907   438.569798  \n",
              "min     -1.000000    0.000000   221.000000   150.000000  \n",
              "25%     -1.000000    0.000000   653.250000   626.500000  \n",
              "50%    113.000000    1.000000   881.000000   818.000000  \n",
              "75%    324.000000    1.000000  1190.000000  1164.000000  \n",
              "max    857.000000    1.000000  4255.000000  3130.000000  \n",
              "\n",
              "[8 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ceaaa8b5-4140-4166-9cd4-67ff2f3d394f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>censor</th>\n",
              "      <th>event</th>\n",
              "      <th>age</th>\n",
              "      <th>wtkg</th>\n",
              "      <th>hemo</th>\n",
              "      <th>homo</th>\n",
              "      <th>drugs</th>\n",
              "      <th>karnof</th>\n",
              "      <th>oprior</th>\n",
              "      <th>z30</th>\n",
              "      <th>...</th>\n",
              "      <th>gender</th>\n",
              "      <th>str2</th>\n",
              "      <th>strat</th>\n",
              "      <th>symptom</th>\n",
              "      <th>cd40</th>\n",
              "      <th>cd420</th>\n",
              "      <th>cd496</th>\n",
              "      <th>r</th>\n",
              "      <th>cd80</th>\n",
              "      <th>cd820</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "      <td>532.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.340226</td>\n",
              "      <td>801.236842</td>\n",
              "      <td>35.225564</td>\n",
              "      <td>76.061855</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>0.640977</td>\n",
              "      <td>0.118421</td>\n",
              "      <td>95.432331</td>\n",
              "      <td>0.030075</td>\n",
              "      <td>0.546992</td>\n",
              "      <td>...</td>\n",
              "      <td>0.812030</td>\n",
              "      <td>0.580827</td>\n",
              "      <td>1.981203</td>\n",
              "      <td>0.167293</td>\n",
              "      <td>353.204887</td>\n",
              "      <td>336.139098</td>\n",
              "      <td>173.146617</td>\n",
              "      <td>0.603383</td>\n",
              "      <td>987.250000</td>\n",
              "      <td>928.214286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.474231</td>\n",
              "      <td>326.887929</td>\n",
              "      <td>8.852094</td>\n",
              "      <td>13.224698</td>\n",
              "      <td>0.269910</td>\n",
              "      <td>0.480165</td>\n",
              "      <td>0.323410</td>\n",
              "      <td>5.981856</td>\n",
              "      <td>0.170955</td>\n",
              "      <td>0.498255</td>\n",
              "      <td>...</td>\n",
              "      <td>0.391056</td>\n",
              "      <td>0.493888</td>\n",
              "      <td>0.905946</td>\n",
              "      <td>0.373589</td>\n",
              "      <td>114.105253</td>\n",
              "      <td>130.961573</td>\n",
              "      <td>191.455406</td>\n",
              "      <td>0.489656</td>\n",
              "      <td>475.223907</td>\n",
              "      <td>438.569798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>47.401000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>221.000000</td>\n",
              "      <td>150.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>535.750000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>67.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>271.000000</td>\n",
              "      <td>243.750000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>653.250000</td>\n",
              "      <td>626.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>933.500000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>74.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>346.000000</td>\n",
              "      <td>330.500000</td>\n",
              "      <td>113.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>881.000000</td>\n",
              "      <td>818.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1081.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>83.502000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>422.000000</td>\n",
              "      <td>418.000000</td>\n",
              "      <td>324.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1164.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1231.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>149.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>771.000000</td>\n",
              "      <td>909.000000</td>\n",
              "      <td>857.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4255.000000</td>\n",
              "      <td>3130.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ceaaa8b5-4140-4166-9cd4-67ff2f3d394f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ceaaa8b5-4140-4166-9cd4-67ff2f3d394f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ceaaa8b5-4140-4166-9cd4-67ff2f3d394f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-04a05f86-237a-4e05-a288-b50af535ed08\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-04a05f86-237a-4e05-a288-b50af535ed08')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-04a05f86-237a-4e05-a288-b50af535ed08 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Quality Checking\n",
        "col = []\n",
        "missing = []\n",
        "level = []\n",
        "for name in data.columns:\n",
        "\n",
        "    # Missing\n",
        "    missper = data[name].isnull().sum() / data.shape[0]\n",
        "    missing.append(round(missper, 4))\n",
        "\n",
        "    # Leveling\n",
        "    lel = data[name].dropna()\n",
        "    level.append(len(list(set(lel))))\n",
        "\n",
        "    # Columns\n",
        "    col.append(name)\n",
        "\n",
        "summary = pd.concat([pd.DataFrame(col, columns=['name']),\n",
        "                     pd.DataFrame(missing, columns=['Missing Percentage']),\n",
        "                     pd.DataFrame(level, columns=['Level'])], axis=1)\n",
        "\n",
        "drop_col = summary['name'][(summary['Level'] <= 1) | (summary['Missing Percentage'] >= 0.8)]\n",
        "data.drop(columns=drop_col, inplace=True)\n",
        "print(f\"Data Shape : {data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPvMy72oP2g5",
        "outputId": "81a4ba99-be18-46ab-ee48-a94836bba069"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shape : (532, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X's & Y Split\n",
        "Y = data['censor']\n",
        "X = data.drop(columns=['censor'])"
      ],
      "metadata": {
        "id": "MVAfmOmOP8n6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = list(range(X.shape[0]))\n",
        "train_idx, valid_idx = train_test_split(idx, test_size=0.3, random_state=2021)\n",
        "print(f\"# of Train data : {len(train_idx)}\")\n",
        "print(f\"# of valid data : {len(valid_idx)}\")\n",
        "print(f\"# of Train data Y : {Counter(Y.iloc[train_idx])}\")\n",
        "print(f\"# of valid data Y : {Counter(Y.iloc[valid_idx])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOsO7wGQP_Hf",
        "outputId": "326141b6-5fcc-4a32-e87e-0ef42ad1033f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Train data : 372\n",
            "# of valid data : 160\n",
            "# of Train data Y : Counter({0: 241, 1: 131})\n",
            "# of valid data Y : Counter({0: 110, 1: 50})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGBM Parameters\n",
        "  - Package : https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
        "  - learning_rate : GBM에서 shrinking 하는 것과 같은 것\n",
        "  - reg_lambda : L2 regularization term on weights (analogous to Ridge regression)\n",
        "  - reg_alpha : L1 regularization term on weight (analogous to Lasso regression)\n",
        "  - objective\n",
        "        objective 🔗︎, default = regression, type = enum, options: regression, regression_l1, huber, fair, poisson, quantile, mape, gamma, tweedie, binary, multiclass, multiclassova, cross_entropy, cross_entropy_lambda, lambdarank, rank_xendcg, aliases: objective_type, app, application, loss\n",
        "\n",
        "  - eval_metric [ default according to objective ]\n",
        "    - The metric to be used for validation data.\n",
        "    - The default values are rmse for regression and error for classification.\n",
        "    - Typical values are:\n",
        "        -    rmse – root mean square error\n",
        "        -    mae – mean absolute error\n",
        "        -    logloss – negative log-likelihood\n",
        "        -    error – Binary classification error rate (0.5 threshold)\n",
        "        -    merror – Multiclass classification error rate\n",
        "        -    mlogloss – Multiclass logloss\n",
        "        -    auc: Area under the curve\n",
        "\n",
        "## LightGBM\n",
        "\n",
        "  - Hyperparameter tuning\n",
        "  - n_estimators, learning_rate, max_depth, reg_alpha\n",
        "  - LightGBM은 Hyperparam이 굉장히 많은 알고리즘 중에 하나임\n",
        "  - 위에 4가지만 잘 조정해도 좋은 결과를 얻을 수 있음"
      ],
      "metadata": {
        "id": "7maVwYEFQC6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n_estimators\n",
        "n_tree = [5, 10, 20]\n",
        "# learning_rate\n",
        "l_rate = [0.1, 0.3]\n",
        "# max_depth\n",
        "m_depth = [3, 5]\n",
        "# reg_alpha\n",
        "L1_norm = [0.1, 0.3, 0.5]\n",
        "\n",
        "# Modeling\n",
        "save_n = []\n",
        "save_l = []\n",
        "save_m = []\n",
        "save_L1 = []\n",
        "f1_score_ = []\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "for n in n_tree:\n",
        "    for l in l_rate:\n",
        "        for m in m_depth:\n",
        "            for L1 in L1_norm:\n",
        "\n",
        "                print(\">>> {} <<<\".format(cnt))\n",
        "                cnt +=1\n",
        "                print(\"n_estimators : {}, learning_rate : {}, max_depth : {}, reg_alpha : {}\".format(n, l, m, L1))\n",
        "                model = LGBMClassifier(n_estimators=n, learning_rate=l,\n",
        "                                       max_depth=m, reg_alpha=L1,\n",
        "                                       n_jobs=-1, objective='cross_entropy')\n",
        "                model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
        "\n",
        "\n",
        "                # Train Acc\n",
        "                y_pre_train = model.predict(X.iloc[train_idx])\n",
        "                cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
        "                print(\"Train Confusion Matrix\")\n",
        "                print(cm_train)\n",
        "                print(\"Train Acc : {}\".format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "                print(\"Train F1-Score : {}\".format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
        "\n",
        "                # Test Acc\n",
        "                y_pre_test = model.predict(X.iloc[valid_idx])\n",
        "                cm_test = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n",
        "                print(\"Test Confusion Matrix\")\n",
        "                print(cm_test)\n",
        "                print(\"TesT Acc : {}\".format((cm_test[0,0] + cm_test[1,1])/cm_test.sum()))\n",
        "                print(\"Test F1-Score : {}\".format(f1_score(Y.iloc[valid_idx], y_pre_test)))\n",
        "                print(\"-----------------------------------------------------------------------\")\n",
        "                print(\"-----------------------------------------------------------------------\")\n",
        "                save_n.append(n)\n",
        "                save_l.append(l)\n",
        "                save_m.append(m)\n",
        "                save_L1.append(L1)\n",
        "                f1_score_.append(f1_score(Y.iloc[valid_idx], y_pre_test))\n",
        "\n",
        "\n",
        "                #joblib.dump(model, './LightGBM_model/Result_{}_{}_{}_{}_{}.pkl'.format(n, l, m, L1, round(save_acc[-1], 4)))\n",
        "                #gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7YItDD0QBAY",
        "outputId": "ff7b8c54-193b-4066-aa9d-5b90fd6626a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 0 <<<\n",
            "n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.328645 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[237   4]\n",
            " [ 55  76]]\n",
            "Train Acc : 0.8413978494623656\n",
            "Train F1-Score : 0.7203791469194314\n",
            "Test Confusion Matrix\n",
            "[[103   7]\n",
            " [ 10  40]]\n",
            "TesT Acc : 0.89375\n",
            "Test F1-Score : 0.8247422680412372\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 1 <<<\n",
            "n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002107 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[237   4]\n",
            " [ 54  77]]\n",
            "Train Acc : 0.8440860215053764\n",
            "Train F1-Score : 0.7264150943396226\n",
            "Test Confusion Matrix\n",
            "[[102   8]\n",
            " [ 10  40]]\n",
            "TesT Acc : 0.8875\n",
            "Test F1-Score : 0.816326530612245\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 2 <<<\n",
            "n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000105 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[237   4]\n",
            " [ 54  77]]\n",
            "Train Acc : 0.8440860215053764\n",
            "Train F1-Score : 0.7264150943396226\n",
            "Test Confusion Matrix\n",
            "[[102   8]\n",
            " [ 10  40]]\n",
            "TesT Acc : 0.8875\n",
            "Test F1-Score : 0.816326530612245\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 3 <<<\n",
            "n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[237   4]\n",
            " [ 44  87]]\n",
            "Train Acc : 0.8709677419354839\n",
            "Train F1-Score : 0.7837837837837838\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[101   9]\n",
            " [  9  41]]\n",
            "TesT Acc : 0.8875\n",
            "Test F1-Score : 0.82\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 4 <<<\n",
            "n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158907 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[236   5]\n",
            " [ 50  81]]\n",
            "Train Acc : 0.8521505376344086\n",
            "Train F1-Score : 0.7465437788018434\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[101   9]\n",
            " [ 10  40]]\n",
            "TesT Acc : 0.88125\n",
            "Test F1-Score : 0.8080808080808082\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 5 <<<\n",
            "n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000123 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[236   5]\n",
            " [ 44  87]]\n",
            "Train Acc : 0.8682795698924731\n",
            "Train F1-Score : 0.7802690582959643\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[101   9]\n",
            " [  9  41]]\n",
            "TesT Acc : 0.8875\n",
            "Test F1-Score : 0.82\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 6 <<<\n",
            "n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013102 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[227  14]\n",
            " [ 17 114]]\n",
            "Train Acc : 0.9166666666666666\n",
            "Train F1-Score : 0.8803088803088803\n",
            "Test Confusion Matrix\n",
            "[[95 15]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.9\n",
            "Test F1-Score : 0.8596491228070174\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 7 <<<\n",
            "n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[228  13]\n",
            " [ 19 112]]\n",
            "Train Acc : 0.9139784946236559\n",
            "Train F1-Score : 0.875\n",
            "Test Confusion Matrix\n",
            "[[96 14]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.90625\n",
            "Test F1-Score : 0.8672566371681417\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 8 <<<\n",
            "n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000111 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[228  13]\n",
            " [ 17 114]]\n",
            "Train Acc : 0.9193548387096774\n",
            "Train F1-Score : 0.883720930232558\n",
            "Test Confusion Matrix\n",
            "[[96 14]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.90625\n",
            "Test F1-Score : 0.8672566371681417\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 9 <<<\n",
            "n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[232   9]\n",
            " [ 20 111]]\n",
            "Train Acc : 0.9220430107526881\n",
            "Train F1-Score : 0.8844621513944223\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[95 15]\n",
            " [ 4 46]]\n",
            "TesT Acc : 0.88125\n",
            "Test F1-Score : 0.8288288288288288\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 10 <<<\n",
            "n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000105 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[232   9]\n",
            " [ 17 114]]\n",
            "Train Acc : 0.9301075268817204\n",
            "Train F1-Score : 0.8976377952755905\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[94 16]\n",
            " [ 5 45]]\n",
            "TesT Acc : 0.86875\n",
            "Test F1-Score : 0.8108108108108109\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 11 <<<\n",
            "n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000095 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[231  10]\n",
            " [ 21 110]]\n",
            "Train Acc : 0.9166666666666666\n",
            "Train F1-Score : 0.8764940239043825\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[97 13]\n",
            " [ 3 47]]\n",
            "TesT Acc : 0.9\n",
            "Test F1-Score : 0.8545454545454546\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 12 <<<\n",
            "n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[233   8]\n",
            " [ 36  95]]\n",
            "Train Acc : 0.8817204301075269\n",
            "Train F1-Score : 0.811965811965812\n",
            "Test Confusion Matrix\n",
            "[[98 12]\n",
            " [ 4 46]]\n",
            "TesT Acc : 0.9\n",
            "Test F1-Score : 0.851851851851852\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 13 <<<\n",
            "n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000210 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[233   8]\n",
            " [ 36  95]]\n",
            "Train Acc : 0.8817204301075269\n",
            "Train F1-Score : 0.811965811965812\n",
            "Test Confusion Matrix\n",
            "[[98 12]\n",
            " [ 4 46]]\n",
            "TesT Acc : 0.9\n",
            "Test F1-Score : 0.851851851851852\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 14 <<<\n",
            "n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008078 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[233   8]\n",
            " [ 33  98]]\n",
            "Train Acc : 0.8897849462365591\n",
            "Train F1-Score : 0.8270042194092826\n",
            "Test Confusion Matrix\n",
            "[[98 12]\n",
            " [ 5 45]]\n",
            "TesT Acc : 0.89375\n",
            "Test F1-Score : 0.8411214953271027\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 15 <<<\n",
            "n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[232   9]\n",
            " [ 28 103]]\n",
            "Train Acc : 0.9005376344086021\n",
            "Train F1-Score : 0.8477366255144032\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[97 13]\n",
            " [ 4 46]]\n",
            "TesT Acc : 0.89375\n",
            "Test F1-Score : 0.8440366972477064\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 16 <<<\n",
            "n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000113 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[233   8]\n",
            " [ 25 106]]\n",
            "Train Acc : 0.9112903225806451\n",
            "Train F1-Score : 0.8653061224489796\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[96 14]\n",
            " [ 4 46]]\n",
            "TesT Acc : 0.8875\n",
            "Test F1-Score : 0.8363636363636363\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 17 <<<\n",
            "n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082052 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[232   9]\n",
            " [ 28 103]]\n",
            "Train Acc : 0.9005376344086021\n",
            "Train F1-Score : 0.8477366255144032\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[97 13]\n",
            " [ 4 46]]\n",
            "TesT Acc : 0.89375\n",
            "Test F1-Score : 0.8440366972477064\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 18 <<<\n",
            "n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[227  14]\n",
            " [ 12 119]]\n",
            "Train Acc : 0.9301075268817204\n",
            "Train F1-Score : 0.9015151515151515\n",
            "Test Confusion Matrix\n",
            "[[92 18]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.88125\n",
            "Test F1-Score : 0.8376068376068375\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 19 <<<\n",
            "n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018027 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[229  12]\n",
            " [ 10 121]]\n",
            "Train Acc : 0.9408602150537635\n",
            "Train F1-Score : 0.9166666666666667\n",
            "Test Confusion Matrix\n",
            "[[92 18]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.88125\n",
            "Test F1-Score : 0.8376068376068375\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 20 <<<\n",
            "n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[229  12]\n",
            " [ 13 118]]\n",
            "Train Acc : 0.9327956989247311\n",
            "Train F1-Score : 0.9042145593869731\n",
            "Test Confusion Matrix\n",
            "[[92 18]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.88125\n",
            "Test F1-Score : 0.8376068376068375\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 21 <<<\n",
            "n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[233   8]\n",
            " [ 12 119]]\n",
            "Train Acc : 0.946236559139785\n",
            "Train F1-Score : 0.9224806201550387\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[98 12]\n",
            " [ 3 47]]\n",
            "TesT Acc : 0.90625\n",
            "Test F1-Score : 0.8623853211009174\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 22 <<<\n",
            "n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000112 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[234   7]\n",
            " [ 10 121]]\n",
            "Train Acc : 0.9543010752688172\n",
            "Train F1-Score : 0.9343629343629344\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[96 14]\n",
            " [ 4 46]]\n",
            "TesT Acc : 0.8875\n",
            "Test F1-Score : 0.8363636363636363\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 23 <<<\n",
            "n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000100 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[234   7]\n",
            " [ 11 120]]\n",
            "Train Acc : 0.9516129032258065\n",
            "Train F1-Score : 0.9302325581395349\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[96 14]\n",
            " [ 4 46]]\n",
            "TesT Acc : 0.8875\n",
            "Test F1-Score : 0.8363636363636363\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 24 <<<\n",
            "n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002344 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[229  12]\n",
            " [ 17 114]]\n",
            "Train Acc : 0.9220430107526881\n",
            "Train F1-Score : 0.8871595330739299\n",
            "Test Confusion Matrix\n",
            "[[95 15]\n",
            " [ 2 48]]\n",
            "TesT Acc : 0.89375\n",
            "Test F1-Score : 0.8495575221238937\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 25 <<<\n",
            "n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000122 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[229  12]\n",
            " [ 14 117]]\n",
            "Train Acc : 0.9301075268817204\n",
            "Train F1-Score : 0.9\n",
            "Test Confusion Matrix\n",
            "[[94 16]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.89375\n",
            "Test F1-Score : 0.8521739130434782\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 26 <<<\n",
            "n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000102 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[228  13]\n",
            " [ 13 118]]\n",
            "Train Acc : 0.9301075268817204\n",
            "Train F1-Score : 0.9007633587786259\n",
            "Test Confusion Matrix\n",
            "[[94 16]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.89375\n",
            "Test F1-Score : 0.8521739130434782\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 27 <<<\n",
            "n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000124 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[233   8]\n",
            " [ 15 116]]\n",
            "Train Acc : 0.9381720430107527\n",
            "Train F1-Score : 0.9098039215686274\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[95 15]\n",
            " [ 3 47]]\n",
            "TesT Acc : 0.8875\n",
            "Test F1-Score : 0.8392857142857143\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 28 <<<\n",
            "n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000100 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[232   9]\n",
            " [ 14 117]]\n",
            "Train Acc : 0.9381720430107527\n",
            "Train F1-Score : 0.9105058365758756\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[95 15]\n",
            " [ 2 48]]\n",
            "TesT Acc : 0.89375\n",
            "Test F1-Score : 0.8495575221238937\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 29 <<<\n",
            "n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101055 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[232   9]\n",
            " [ 17 114]]\n",
            "Train Acc : 0.9301075268817204\n",
            "Train F1-Score : 0.8976377952755905\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[96 14]\n",
            " [ 3 47]]\n",
            "TesT Acc : 0.89375\n",
            "Test F1-Score : 0.8468468468468469\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 30 <<<\n",
            "n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140072 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[236   5]\n",
            " [  6 125]]\n",
            "Train Acc : 0.9704301075268817\n",
            "Train F1-Score : 0.9578544061302683\n",
            "Test Confusion Matrix\n",
            "[[89 21]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.8625\n",
            "Test F1-Score : 0.8166666666666667\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 31 <<<\n",
            "n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[233   8]\n",
            " [  6 125]]\n",
            "Train Acc : 0.9623655913978495\n",
            "Train F1-Score : 0.9469696969696969\n",
            "Test Confusion Matrix\n",
            "[[91 19]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.875\n",
            "Test F1-Score : 0.8305084745762712\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 32 <<<\n",
            "n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003082 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[234   7]\n",
            " [  8 123]]\n",
            "Train Acc : 0.9596774193548387\n",
            "Train F1-Score : 0.9425287356321839\n",
            "Test Confusion Matrix\n",
            "[[93 17]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.8875\n",
            "Test F1-Score : 0.8448275862068965\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 33 <<<\n",
            "n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000110 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[239   2]\n",
            " [  2 129]]\n",
            "Train Acc : 0.989247311827957\n",
            "Train F1-Score : 0.9847328244274809\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[94 16]\n",
            " [ 2 48]]\n",
            "TesT Acc : 0.8875\n",
            "Test F1-Score : 0.8421052631578947\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 34 <<<\n",
            "n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[237   4]\n",
            " [  0 131]]\n",
            "Train Acc : 0.989247311827957\n",
            "Train F1-Score : 0.9849624060150376\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[93 17]\n",
            " [ 2 48]]\n",
            "TesT Acc : 0.88125\n",
            "Test F1-Score : 0.8347826086956522\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            ">>> 35 <<<\n",
            "n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Train Confusion Matrix\n",
            "[[237   4]\n",
            " [  1 130]]\n",
            "Train Acc : 0.9865591397849462\n",
            "Train F1-Score : 0.9811320754716981\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Test Confusion Matrix\n",
            "[[92 18]\n",
            " [ 4 46]]\n",
            "TesT Acc : 0.8625\n",
            "Test F1-Score : 0.8070175438596492\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\">>> {} <<<\\nBest Test f1-score : {}\\nBest n_estimators : {}\\nBest Learning Rate : {}\\nBest Max_depth : {}\\nBest L1-norm : {}\".format(np.argmax(f1_score_),\n",
        "                                                                                                                                            f1_score_[np.argmax(f1_score_)],\n",
        "                                                                                                                                            save_n[np.argmax(f1_score_)],\n",
        "                                                                                                                                            save_l[np.argmax(f1_score_)],\n",
        "                                                                                                                                            save_m[np.argmax(f1_score_)],\n",
        "                                                                                                                                            save_L1[np.argmax(f1_score_)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QBoj5xUQLJx",
        "outputId": "d9b550fd-3acd-4720-e76d-b785dd7a660c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 7 <<<\n",
            "Best Test f1-score : 0.8672566371681417\n",
            "Best n_estimators : 5\n",
            "Best Learning Rate : 0.3\n",
            "Best Max_depth : 3\n",
            "Best L1-norm : 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = LGBMClassifier(n_estimators=save_n[np.argmax(f1_score_)], learning_rate=save_l[np.argmax(f1_score_)],\n",
        "                           max_depth=save_m[np.argmax(f1_score_)], reg_alpha=save_L1[np.argmax(f1_score_)], objective='cross_entropy',\n",
        "                           random_state=119)\n",
        "best_model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
        "\n",
        "# Train Acc\n",
        "y_pre_train = best_model.predict(X.iloc[train_idx])\n",
        "cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
        "print(\"Train Confusion Matrix\")\n",
        "print(cm_train)\n",
        "print(\"Train Acc : {}\".format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
        "print(\"Train F1-Score : {}\".format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
        "\n",
        "# Test Acc\n",
        "y_pre_test = best_model.predict(X.iloc[valid_idx])\n",
        "cm_test = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n",
        "print(\"Test Confusion Matrix\")\n",
        "print(cm_test)\n",
        "print(\"TesT Acc : {}\".format((cm_test[0,0] + cm_test[1,1])/cm_test.sum()))\n",
        "print(\"Test F1-Score : {}\".format(f1_score(Y.iloc[valid_idx], y_pre_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yimr78esQQdb",
        "outputId": "c04c48a9-2afd-47ff-e7bd-8f709bac527a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
            "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 905\n",
            "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
            "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
            "[LightGBM] [Info] Start training from score -0.609600\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train Confusion Matrix\n",
            "[[228  13]\n",
            " [ 19 112]]\n",
            "Train Acc : 0.9139784946236559\n",
            "Train F1-Score : 0.875\n",
            "Test Confusion Matrix\n",
            "[[96 14]\n",
            " [ 1 49]]\n",
            "TesT Acc : 0.90625\n",
            "Test F1-Score : 0.8672566371681417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map = pd.DataFrame(sorted(zip(best_model.feature_importances_, X.columns), reverse=True), columns=['Score', 'Feature'])\n",
        "print(feature_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7tFvAUEQUp1",
        "outputId": "bb64d937-552d-4671-da0d-f7aae638cb71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Score  Feature\n",
            "0      10    event\n",
            "1       4    cd496\n",
            "2       4    cd420\n",
            "3       4     cd40\n",
            "4       3  preanti\n",
            "5       2     wtkg\n",
            "6       2     race\n",
            "7       1      z30\n",
            "8       1      age\n",
            "9       0  symptom\n",
            "10      0    strat\n",
            "11      0     str2\n",
            "12      0        r\n",
            "13      0   oprior\n",
            "14      0   karnof\n",
            "15      0     homo\n",
            "16      0     hemo\n",
            "17      0   gender\n",
            "18      0    drugs\n",
            "19      0    cd820\n",
            "20      0     cd80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importance Score Top 10\n",
        "feature_map_10 = feature_map.iloc[:10]\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.barplot(x=\"Score\", y=\"Feature\", data=feature_map_10.sort_values(by=\"Score\", ascending=False), errwidth=40, palette='Pastel1')\n",
        "plt.title('XGBoost Importance Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "cONusO1HQbQE",
        "outputId": "ac535350-1253-4d20-e133-1c7730742dae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8vip-ROQgOP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}